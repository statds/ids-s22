{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236671d5",
   "metadata": {},
   "source": [
    "# Bagging versus Boosting\n",
    "\n",
    "An ensemble method combines multiple individual models in order to\n",
    "produce predictions with better properties. It has been reported that\n",
    "an ensemble is often more accurate than any of the single member\n",
    "in the ensemble {cite:ps}`opitz1999popular, buhlmann2012bagging`.\n",
    "\n",
    "\n",
    "The bias-variance tradeoff is the key motivation. Each single model\n",
    "(learner) may have high bias or higher variance or both. An ensemble\n",
    "method seeks to reduce bias and/or variance of such weak learners by\n",
    "combining several of them together in order to create a strong learner\n",
    "(or ensemble model) that achieves better performances.\n",
    "\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Bagging stands for bootstrap aggregation. \n",
    "In bagging, a collection of bootstrap samples of the data in a\n",
    "training dataset are generated; a model (e.g., decision tree;\n",
    "regression) is fit to each of the bootstrap sample independently; the\n",
    "predictions of all the members in the ensemble are aggregated (e.g.,\n",
    "majority vote). Bagging reduces the variance of the final prediction\n",
    "through aggregation.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Random forest is a bagging approach with decision trees as individual\n",
    "ensemble members, except that the features used for each tree are a\n",
    "random subsample of all the features. The purpose of resampling the\n",
    "features it to make the trees less dependent and more robust to\n",
    "missing data.\n",
    "\n",
    "Tuning parameters for a random forest include:\n",
    "1. number of trees;\n",
    "1. number of features;\n",
    "1. tree depth.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boostting is a sequential approach where the training of a model\n",
    "learner at a given step applies to \"residuals\" from the model fitted\n",
    "at the previous steps. This way, the current step improves on data\n",
    "points where the cumulative performance is not good. Boosting produces\n",
    "an ensemble model that in general less biased than the weak learners\n",
    "that compose it. \n",
    "\n",
    "Unlike bagging, boosting cannot be done in parallel.\n",
    "\n",
    "Boosting methods differ on how they create and aggregate the weak\n",
    "learners during the sequential process.\n",
    "\n",
    "See [Joseph Rocca's\n",
    "post](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205);\n",
    "[Jason Brownlee's\n",
    "blog](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).\n",
    "\n",
    "\n",
    "### Adaptive Boosting (AdaBoost)\n",
    "Adaptive boosting updates the weights attached to each of the training\n",
    "dataset observations. The ensemble model is a weighted sum of the weak\n",
    "learners. Instead of trying to solve it in one single shot (finding\n",
    "all the coefficients and weak learners that give the best overall\n",
    "additive model), an iterative optimisation process is\n",
    "much more tractable, even if it can lead to a sub-optimal solution.\n",
    "\n",
    "\n",
    "For illustration, consider a binary classification problem, with $N$\n",
    "observations and a given family of weak models. To initialize, all the\n",
    "observations have the same weights $1/N$. Repeat over each weak model:\n",
    "+ fit the best possible weak model with the current observations\n",
    "  weights;\n",
    "+ compute the value of the update coefficient that is some kind of\n",
    "  scalar evaluation metric of the weak learner that indicates how much\n",
    "  this weak learner should be taken into account into the ensemble\n",
    "  model;\n",
    "+ update the strong learner by adding the new weak learner multiplied\n",
    "  by its update coefficient;\n",
    "+ compute new observations weights that\n",
    "  expresse which observations we would like to focus on at the next\n",
    "  iteration (weights of observations wrongly predicted by the\n",
    "  aggregated model increase and weights of the correctly predicted\n",
    "  observations decrease).\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Gradient boosting updates the dataset in eaching step. It casts the\n",
    "problem into a gradient descent one: at each iteration we fit a weak\n",
    "learner to the negative gradients of the current fitting error\n",
    "(pseudo-residuals) with respect to the current ensemble model. In the\n",
    "regression setting, the negative gradient is the residual from the\n",
    "current ensemble model.\n",
    "\n",
    "For illustration, continue with the binary classification\n",
    "problem. Initialize the pseudo-residuals as the observation outcomes.\n",
    "Repeat over the following steps:\n",
    "+ fit the best possible weak learner to pseudo-residuals (approximate\n",
    "  the negative gradients with respect to the current strong learner);\n",
    "+ compute the value of the optimal step size that defines by how much\n",
    "  we update the ensemble model in the direction of the new weak\n",
    "  learner;\n",
    "+ update the ensemble model by adding the new weak learner multiplied\n",
    "  by the step size (make a step of gradient descent);\n",
    "+ compute new pseudo-residuals that indicate, for each observation, in\n",
    "  which direction we would like to update next the ensemble model\n",
    "  predictions.\n",
    "\n",
    "Gradient boosting uses a gradient descent approach and can be easily\n",
    "adapted to a large number of loss functions. It can be considered as a\n",
    "generalization of adaboost to arbitrary differentiable loss functions.\n",
    "\n",
    "\n",
    "#### Stochastic gradient boosting\n",
    "\n",
    "Stochastic gradient boosting (SGD) is a stochastic version of gradient\n",
    "boosting. At each iteration, a subsample of the training data is drawn\n",
    "at random (without replacement) from the full training dataset. The\n",
    "randomly selected subsample is then used, instead of the full sample,\n",
    "to fit the base learner. It reduce the dependence between the trees\n",
    "in the sequence in gradient boosting models.\n",
    "\n",
    "\n",
    "\n",
    "#### Extreme gradient boosting\n",
    "Extreme gradient boosting is a specific implementation of the gradient\n",
    "boosting method which uses more accurate approximations to find the\n",
    "best tree model. It employs a number of nifty tricks that make it\n",
    "exceptionally successful, particularly with structured data. \n",
    "\n",
    "+ computing second-order gradients, i.e. second partial derivatives of\n",
    "  the loss function (similar to Newtonâ€™s method), which provides more\n",
    "  information about the direction of gradients and how to get to the\n",
    "  minimum of our loss function. While regular gradient boosting uses\n",
    "  the loss function of our base model (e.g. decision tree) as a proxy\n",
    "  for minimizing the error of the overall model, XGBoost uses the 2nd\n",
    "  order derivative as an approximation.\n",
    "+ advanced regularization (L1 & L2), which improves model\n",
    "  generalization.\n",
    "+ parallelized."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "source_map": [
   12
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}